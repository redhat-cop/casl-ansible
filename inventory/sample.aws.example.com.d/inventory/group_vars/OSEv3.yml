---

# The username Ansible should use to access the instances with
ansible_user: ec2-user

# Should Ansible use "become" to gain elevated privileges (i.e.: root)
ansible_become: true

# Node Labels to use with the CASL tools
openshift_node_labels: "{{ ec2_tag_node_labels }}"

# CNS relative vars - Uncommented to automatically deploy CNS - 'cns_deploy' from all.yml must be 'true' in that case
# openshift_storage_glusterfs_namespace: glusterfs
# openshift_storage_glusterfs_name: cns

# Uncommented to automatically integrate OCP with AWS so EBS can be used as Dynamic Storage backend
openshift_cloudprovider_kind: aws
openshift_cloudprovider_aws_access_key: "{{ lookup('env','AWS_ACCESS_KEY_ID') }}"
openshift_cloudprovider_aws_secret_key: "{{ lookup('env','AWS_SECRET_ACCESS_KEY') }}"
openshift_clusterid: "{{ env_id }}"


# OpenShift Specific Configuration Options
# - Check the official OpenShift documentation for more details
deployment_type: openshift-enterprise
openshift_deployment_type: openshift-enterprise
containerized: false

### OCP version to install
openshift_release: v3.9

osm_default_node_selector: 'region=primary'
osm_use_cockpit: true
osm_cockpit_plugins:
- 'cockpit-kubernetes'

# Enable the Multi-Tenant plugin
os_sdn_network_plugin_name: 'redhat/openshift-ovs-multitenant'

# OpenShift FQDNs, DNS, App domain specific configurations
openshift_master_cluster_method: native
openshift_master_default_subdomain: "apps.{{ env_id }}.{{ dns_domain }}"
openshift_master_cluster_hostname: "console.internal.{{ env_id }}.{{ dns_domain }}"
openshift_master_cluster_public_hostname: "console.{{ env_id }}.{{ dns_domain }}"


# Deploy Logging with dynamic storage
#openshift_logging_install_logging: false
#openshift_logging_es_pvc_dynamic: true
#openshift_logging_es_pvc_size: 40G
#openshift_logging_curator_default_days: 1

# Deploy Metrics with dynamic storage
#openshift_metrics_install_metrics: false
#openshift_metrics_cassandra_storage_type: dynamic
#openshift_metrics_cassandra_pvc_size: 40G
#openshift_metrics_duration: 2

# HTPASSWD Identity Provider
# - update to other types of auth providers if necessary (i.e: LDAP, OAuth, ...)
openshift_master_identity_providers:
 - 'name': 'htpasswd_auth'
   'login': 'true'
   'challenge': 'true'
   'kind': 'HTPasswdPasswordIdentityProvider'
   'filename': '/etc/origin/master/htpasswd'

# Uncommented to automatically create a set of test users with the above
# HTPASSWD Identity Provider
#create_users:
#  num_users: 5
#  prefix: 'rdu-user'
#  passwd_file: '/etc/origin/master/htpasswd'
#  password: 'rdu-sample'


# OpenShift Node specific parameters
openshift_node_groups:
  - name: node-config-master 
    labels:
      - 'node-role.kubernetes.io/master=true' 
    edits: 
    - key: kubeletArguments.kube-reserved
      value: [ 'cpu={{ ansible_processor_vcpus * 50 }}m', 'memory={{ ansible_processor_vcpus * 50 }}M' ]
    - key: kubeletArguments.system-reserved
      value: [ 'cpu={{ ansible_processor_vcpus * 50 }}m', 'memory={{ ansible_processor_vcpus * 100 }}M' ]      
  - name: node-config-infra
    labels:
      - 'node-role.kubernetes.io/infra=true'
    edits:
    - key: kubeletArguments.kube-reserved
      value: [ 'cpu={{ ansible_processor_vcpus * 50 }}m', 'memory={{ ansible_processor_vcpus * 50 }}M' ]
    - key: kubeletArguments.system-reserved
      value: [ 'cpu={{ ansible_processor_vcpus * 50 }}m', 'memory={{ ansible_processor_vcpus * 100 }}M' ]
  - name: node-config-compute
    labels:
      - 'node-role.kubernetes.io/compute=true'
    edits: 
    - key: kubeletArguments.kube-reserved
      value: [ 'cpu={{ ansible_processor_vcpus * 50 }}m', 'memory={{ ansible_processor_vcpus * 50 }}M' ]
    - key: kubeletArguments.system-reserved
      value: [ 'cpu={{ ansible_processor_vcpus * 50 }}m', 'memory={{ ansible_processor_vcpus * 100 }}M' ]

# Open required ports on IPTables for Prometheus Node Exporters & Router Statistics
openshift_node_open_ports:
  - service: "router stats port"
    port: "1936/tcp"
  - service: "prometheus node exporter"
    port: "9100/tcp"